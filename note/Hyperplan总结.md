# Hyperplan总结

这是一个法国南特大学的学生项目，项目组包括我在内有3名成员以及1名以及毕业的创业师兄。最开始师兄是想要我们使用他开发的一个接口来结合实际应用做一个demo，内容是包含实习内容邮件的分类。

这个接口叫hyperplan，其最初目的是想通过这个接口，让工程师进行内部具体整个pipeline的开发，留出API对接上他的hyperplan，然后让data scientist就是数据科学家进行模型调优这样，我一开始也觉得这个想法是挺蛋疼的，但是他那边坚持要弄这么个东西，我们也就只好跟着做了。

最开始他的需求是说我们提取一下邮件的关键字，然后中间需求又改了一下说是要做邮件内容分类这样。

最后他觉得他开发的这个接口实在没有太大的意义，所以在我们完成了我们的工作之后，他就暂停维护了。

---

内容：把学校邮箱中的实习邮件下载下来，里面的附件有多种形式，使用不同的Python库对文件进行解读，保存为文本的形式。然后对文本进行分类：

---

1. **tf-idf:** 这个算法本质上是寻找文本的关键词，也是企业员工最开始的需求。term frequency–inverse document frequency。一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词。

   具体实现上：对于每个文本，计算每个词出现的次数。对于所有文本，在去除无意义词之后，计算每个词出现的频率(TF)。然后计算每个词的IDF是多少；相乘得到TF-IDF，按降序排列，取前面的几个词。

   ![](https://pic1.zhimg.com/80/v2-393435b342546a2f1736d1d755adb1cd_720w.jpg)

   ![](https://pic2.zhimg.com/80/v2-1d5c436e04f497544d72fec6909a3fad_720w.jpg)

   如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。log表示对得到的值取对数。

   **TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。**

   **优缺点**：TF-IDF的优点是简单快速，而且容易理解。缺点是有时候用**词频**来衡量文章中的一个词的重要性不够全面，有时候重要的词出现的可能不够多，而且这种计算无法体现位置信息，无法体现词在上下文的重要性。

   

   ---

   

2. **k-means**：　k-means算法需要事先指定簇的个数k，算法开始随机选择k个记录点作为中心点，然后遍历整个数据集的各条记录，将每条记录归到离它最近的中心点所在的簇中，之后以各个簇的记录的均值中心点取代之前的中心点，然后不断迭代，直到收敛。

   具体地：然后他们改了需求，使用存在的训练数据，**将分词后的文本矩阵化**，对数据库中的每个文本进行分类，

   **簇标签生成**：对于得到的结果中的每个文本，我们都有其ground_truth的分类，取其大多数的分类为该结果集的分类。或者我们可以进行人工干预，指定该分类的属性。最后使用这个模型来进行新获取的实习邮件分类。还可以用TFIDF、互信息。

   优缺点：

   - Kmeans聚类是一种自下而上的聚类方法，它的优点是简单、速度快；
   - 缺点是聚类结果与初始中心的选择有关系，且必须提供聚类的数目。
   - Kmeans的第二个缺点是致命的，因为在有些时候，我们不知道样本集将要聚成多少个类别，这种时候kmeans是不适合的。
   - 第一个缺点可以通过多次聚类取最佳结果来解决。
   - 第二个缺点可以通过选取不同的k值，然后选择损失最小的k值。

   ---

   

3. textCNN

   利用卷积神经网络对文本进行分类的算法。

   n维的词向量，m为句子长度，所以每个句子的输入矩阵大小为m*n。对于文本数据，filter不再横向移动，仅仅向下移动。

   对于word2vec，我们借用了别人的代码

   图片

   ![](https://img-blog.csdnimg.cn/20190326141457137.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FzaWFsZWVfYmlyZA==,size_16,color_FFFFFF,t_70)

